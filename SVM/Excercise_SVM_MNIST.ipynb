{"cells":[{"cell_type":"markdown","metadata":{"id":"p7aubF5yGRY4"},"source":["**SVM Multiclass Classification.**\n","\n","SVM does not support multiclass classification natively. Two commonly used approaches that extend SVM for multiclass classification are One-vs-One and One-vs-Rest. In this exercise, we would like you to apply multiclass classification using SVM to classify number 0-10 from MNIST dataset. \n"," \n","Specifically, we would like you to explore the following: \n","\n","1. **[5 scores]** You may randomly select 6000 samples for training and 1000 sample for testing. Ensure that you have chosen the samples evenly from each class. Then, show us the distribution of labels in the selected training and testing samples.\n","\n","2. **[10 scores]** Let's assume that we choose the RBF kernel for SVM. You may separate your training set for tuning and validation. Please show the following results:   \n","*   a. Show the accuracy (or loss ) curves across of the validation set across different kernels and model parameters. \n","*   b. Pick the best set of parameters and verify the final performance on the testing dataset.  \n","\n","3. **[25 scores]** To see the differences between One-vs-one and One-vs-the rest. Let’s observe the positive and negative supports.  \n","*   a. For one-vs-one classification, what is the number of binary classifiers and how is it related to the number of classes? \n","    - Observe the positive and negative supports of the first separation, the last separation, and any where in the middle.\n","*   b. For one-vs-rest classification, same question for the binary classifiers and number of classes. \n","    - Also, observe the positive and negative supports of the first separation,  the last separation, and any where inbetween.\n","    \n","*   c. Can you tell the differences between the observation in (3.a) and (3.b)? \n","    - For each observation, you may plot the mean shapes of the positive and negative supports & the histogram of the labels associated with the positive and negative supports.\n","\n","\n","\n","---\n","\n","Note.\n","\n","To get the full score, you should be able to provide the following plots with resonable results and **with good explaination**:\n","\n","1. SVM_1_MNIST_label_distribution.png  **[5 scores]**  \n","2. SVM_2_ModelSelection.png **[5 scores]** +  your answers **[5 scores]**\n","3. Two sets for one_vs_one plots **[10 scores]**  and one_vs_rest_0/8/x plots **[10 scores]** and your answers **[5 scores]**. The examples of the plot files are as follows: \n","\n","  - SVM_3_mean_positive_support_one_vs_one_0.png\n","  - SVM_3_mean_positive_support_one_vs_one_8.png\n","  - SVM_3_mean_positive_support_one_vs_one_x.png\n","  - SVM_3_mean_negative_support_one_vs_one_0.png\n","  - SVM_3_mean_negative_support_one_vs_one_8.png\n","  - SVM_3_mean_negative_support_one_vs_one_x.png\n","\n","  - SVM_3_MNIST_neg_pos_distribution_one_vs_one_0.png\n","  - SVM_3_MNIST_neg_pos_distribution_one_vs_one_8.png\n","  - SVM_3_MNIST_neg_pos_distribution_one_vs_one_x.png\n","\n","  - SVM_3_positive_support_one_vs_one_0.png\n","  - SVM_3_positive_support_one_vs_one_8.png\n","  - SVM_3_positive_support_one_vs_one_x.png\n","\n","  - SVM_3_negative_support_one_vs_one_0.png\n","  - SVM_3_negative_support_one_vs_one_8.png\n","  - SVM_3_negative_support_one_vs_one_x.png\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4T1HdWOSJp8r"},"outputs":[],"source":["from scipy.stats import mode\n","import numpy as np\n","#from mnist import MNIST\n","from time import time\n","import pandas as pd\n","import os\n","import matplotlib.pyplot as plt\n","import matplotlib \n"," \n","from itertools import chain\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score\n"," \n","from sklearn.model_selection import ParameterGrid\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.multiclass import OneVsRestClassifier\n","import pandas as pd\n","import tensorflow as tf\n","\n","########################################################### \n","####################   Q.1 [5 scores]  ####################\n","########################################################### \n","\n","# Load the MNIST dataset\n","\n","(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","\n","# <<<< Q.1 Sample the data and reshape it into a 2D array, e.g. seq = np.random.randint(0,60000,6000)\n","# [Hint!] Dont forget to reshape train_images and test_images, e.g., train_images[seq,:,:].reshape(-1,28*28)\n","seq_train =  # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","train_samp  = train_images[seq_train,:,:] \n","trlab_samp  = train_labels[seq_train] \n","\n","seq_test =  # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","test_samp   = test_images[seq_test, :,:]\n","tslab_samp  = test_labels[seq_test]  \n","\n","# <<<< Q.1 Sample the distribution of the training and testing labels. \n","# [Hint] You may use `hist, bin= np.histogram(trlab_samp, range=[0,10])`\n","# to get the histogram of the training labels.\n","\n","train_hist, train_bins =  # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","test_hist, test_bins   =  # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","\n","ticks = range(10) \n","width = 0.4\n","fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4)) \n","ax = axes[0]\n","ax.bar(ticks,  , width, label='Training') # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","ax.set_xticks(ticks)\n","ax.set_ylabel('Frequency')\n","ax.set_xlabel('Label')\n","ax.set_title('Training Labels')\n"," \n","ax = axes[1] \n","ax.bar(ticks,  , width, label='Testing') # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","ax.set_xticks(ticks)\n","ax.set_ylabel('Frequency')\n","ax.set_xlabel('Label')\n","ax.set_title('Testing Labels')  \n","fig.savefig(\"SVM_1_MNIST_label_distribution.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K4O8sQlgDTL5"},"outputs":[],"source":["#################################################################\n","#####################   Q.2 [10 scores] #########################\n","#################################################################\n","# 2. Training and model selection\n","\n","from sklearn.model_selection import GridSearchCV, train_test_split\n","from sklearn.metrics import hinge_loss\n","\n","c_list =   #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<                       \n","g_list =   #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< \n","\n","# [Hint] You can also recheck your result with GridSearchCV. For example .... \n","''' \n","gamma_list = [0.01, 0.1, 1]\n","c_list     = [0.001, 0.1, 1, 10, 100]\n","param_grid = {'C': c_list, 'gamma':  gamma_list}\n","\n","# Create a GridSearchCV object with the SVM model, hyperparameters, and custom scoring function\n","svc = SVC(kernel='rbf')\n","grid_search = GridSearchCV(svc, param_grid=param_grid, cv=5, return_train_score=True)\n","\n","# Fit the GridSearchCV object to the training data\n","grid_search.fit(train_samp, trlab_samp)\n","\n","# Get the best hyperparameters and associated training and validation losses\n","best_params = grid_search.best_params_ \n","C = best_params['C']\n","gamma = best_params['gamma']\n","'''\n","\n","sub_train_samp, val_samp, sub_trlab_samp, valab_samp = train_test_split(train_samp, trlab_samp, test_size=0.2)\n","\n","tuning_ = [] \n","\n","for c in c_list:  \n","  for g in g_list:\n","    # <<<<<<<<<<<<<<<< Q.2.A  Train the SVM and compute the accuracy or loss on the training and validation data. \n","    # [Hint] Train the SVM, i.e., svm = SVC(kernel='rbf', C=xx, gamma=xx), for the selected hyperparameters using svm.fit() ... \n","    # and compute the accuracy or loss on the training and validation data. \n","     \n","\n","    # Compute the accuracy (or loss), e.g. accuracy_train = svm.score(sub_train_samp, sub_trlab_samp)...\n","    # alternatively, you can comput the loss, e.g., using hinge_loss(sub_trlab_samp, svm.decision_function(sub_train_samp))\n","    \n","\n","    # Compute the accuracy (or loss), e.g. accuracy_validate = svm.score(val_samp, valab_samp) \n","    # alternatively, you can comput the loss, e.g., using hinge_loss(valab_samp, svm.decision_function(val_samp))\n","\n","    tuning_.append({\"C\":c, \"gamma\":g,   'ACC/Loss_tra' : accuracy_train, 'ACC/Loss_val' : accuracy_validate })\n","    print({\"C\":c, \"gamma\":g,    'ACC/Loss_tra' : accuracy_train, 'ACC/Loss_val' : accuracy_validate })\n","\n","df_tuning = pd.DataFrame(tuning_)\n"," \n","training_acc   = df_tuning['ACC/Loss_tra']\n","validating_acc = df_tuning['ACC/Loss_val']\n","\n","fig = plt.figure(figsize=(5,5)) \n","plt.plot(training_acc, label='training',color='blue', linewidth=2.0)\n","plt.plot(validating_acc, label='validate',color='red', linewidth=2.0)\n","plt.grid(which='major', color='#DDDDDD', linewidth=0.8)\n","plt.grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n","plt.xlabel(\"C, gamma parameters\") \n","plt.legend()\n","plt.ylabel(\"Loss [The lower the better]\") # <<<< If you use Accuracy [the higher the better], please change the y axist to accuracy... if you are using loss, please change the y axis to loss.\n","fig.savefig(\"SVM_2_ModelSelection.png\")\n","\n","# You can also try .... \n","\n","# gamma_list = [0.01, 0.1, 1]\n","# c_list     = [0.001, 0.1, 1, 10, 100]\n","# param_grid = {'C': c_list, 'gamma':  gamma_list}\n","\n","# # Create a GridSearchCV object with the SVM model, hyperparameters, and custom scoring function\n","# svc = SVC(kernel='rbf')\n","# grid_search = GridSearchCV(svc, param_grid=param_grid, cv=5, return_train_score=True)\n","\n","# # Fit the GridSearchCV object to the training data\n","# grid_search.fit(train_samp, trlab_samp)\n","\n","# # Get the best hyperparameters and associated training and validation losses\n","# best_params = grid_search.best_params_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1fx3OlyDDRQK"},"outputs":[],"source":["#################################################################\n","#####################   Q.3 [25 scores] #########################\n","#################################################################\n","# 3. To see the differences between One-vs-one and One-vs-the rest. Let’s observe the positive and negative supports.  \n","#     a. For one-vs-one classification, what is the number of binary classifiers and how is it related to the number of classes? \n","#         - Observe the positive and negative supports of the first separation, the last separation, and any where in the middle.\n","#     b. For one-vs-rest classification, same question for the supports and number of classes. \n","#         - Also, observe the positive and negative supports of the first separation,  the last separation, and any where inbetween.\n","#     c. Can you tell the differences between the observation in (3.a) and (3.b)? \n","#     - For each observation, you may plot the mean shapes of the positive and negative supports & the histogram of \n","#       the labels associated with the positive and negative supports.\n"," \n","best_c = 1 # Please change the number to the best C you found in the previous step\n","best_gamma =  1 # Please change the number to the best C you found in the previous step\n","\n","type_svm =  \"one_vs_one\"  #<<<<<<   Q.3.A,B,C [Please change the type of SVM you want to use. You can choose either \"one_vs_one\" or \"one_vs_rest\"]\n","\n","# Perform the training for SVM classification\n","if type_svm == \"one_vs_one\":\n","  # <<<<<<<< Q.3.A  Train One vs One [SVM model with RBF kernel].  \n","  # [Hint]: svm = SVC(kernel='rbf', C=XX, gamma=XX) and svm.fit(train_samp, trlab_samp)\n","  #\n","\n","elif type_svm == \"one_vs_rest\":\n","  # #<<<<<< Q.3.B Train One vs Rest [SVM model with RBF kernel].   \n","  # [Hint]: Use OneVsRestClassifier function from sklearn.multiclass from svm to ovr_svc = OneVsRestClassifier(svm) and ovr_svc.fit(train_samp, trlab_samp)\n","  # \n","  \n","\n","\n","# Visualize the supports (positive and negative supports)\n","\n","if type_svm == \"one_vs_one\":\n","  dual_coef = svm.dual_coef_\n","  support   = svm.support_\n","\n","  print(\"Number of  binary classifiers: %d\" % dual_coef.shape[0])\n","  print(\"Number of  Support Coefficients: %d\" % dual_coef.shape[1])\n","  \n","  class_i = 0 # <<<<   Q.3.A,C Try the first separation class_i=0, the last separation class_i=8, and any where inbetween (e.g. class_i=3) ...Check with the number of  binary classifiers\n","  separate_i    = class_i\n","  pos_support = support[dual_coef[separate_i,:] > 0]\n","  neg_support = support[dual_coef[separate_i,:] < 0]\n","\n","elif type_svm == \"one_vs_rest\":  \n","   \n","  class_i = 0  #  <<<<  Q.3.B,C  Try the first separation class_i=0, the last separation class_i=8, and any where inbetween (e.g. class_i=3)...Check with the number of binary classifiers \n","\n","  # Get the binary classifiers for each class\n","  binary_clf = ovr_svc.estimators_[class_i] \n","  # get the dual coefficients for class class_i\n","  dual_coef = binary_clf.dual_coef_\n","  support   = binary_clf.support_\n","\n","  print(\"Number of  binary classifiers: %d\" % len(ovr_svc.estimators_))\n","  print(\"Number of  Support Coefficients: %d\" % dual_coef.shape[1])\n","\n","  pos_support = support[dual_coef[0,:] > 0]\n","  neg_support = support[dual_coef[0,:] < 0]\n","\n","\n","print(f\"Number of supports for positive class: {len(pos_support)}\")\n","print(f\"Number of supports for negative class: {len(neg_support)}\")\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OzffA5Q1GRZF"},"outputs":[],"source":["# Plot the mean shapes of the positive supports\n","# [Hint] Use `pos_support` to find the positive support from the traing samples, i.e, train_samp, or from svm.support_vectors_\n","pos_supports     =    #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< <<<<  Q.3.A,B,C\n","label_pos        =    #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< <<<<  Q.3.A,B,C\n","\n","av_train = pos_supports.mean(axis=0).reshape(-1, 28, 28) \n","fig = plt.figure(figsize=(5,5))\n","plt.imshow(av_train.reshape(28, 28), cmap=plt.cm.RdBu)\n","fig.savefig(\"SVM_3_mean_positive_support_%s_%d.png\" % (type_svm,class_i))\n","\n","show_support = min(len(pos_support),len(neg_support))  \n","chosen_sample = np.random.randint(0, show_support,100)\n","\n","ind = 0\n","fig = plt.figure(figsize=(24,50))\n","for i, sample_id in enumerate(chosen_sample):\n","  l1 = plt.subplot(int(len(chosen_sample)/5), 5, i + 1)  \n","  sv_image = pos_supports[sample_id,:]\n","  sv_label = label_pos[sample_id]\n","  l1.imshow(sv_image.reshape(28, 28), cmap=plt.cm.RdBu)\n","  l1.set_xticks(())\n","  l1.set_yticks(())\n","  l1.set_xlabel('Sep/Class %d : Sample %d, label %s' % (class_i, i, str(sv_label))) \n","fig.savefig(\"SVM_3_positive_support_%s_%d.png\" % (type_svm,class_i))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pz6Pyf_Vy5yL"},"outputs":[],"source":["# Plot the mean shapes of the negative supports\n","# [Hint] Use `neg_support` to find the negative supports from the traing samples, i.e, train_samp, or from svm.support_vectors_\n","neg_supports =    #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< <<<<  Q.3.A,B,C\n","label_neg        =     #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< <<<<  Q.3.A,B,C\n","\n","av_train = neg_supports.mean(axis=0).reshape(-1, 28, 28) \n","fig = plt.figure(figsize=(5,5))\n","plt.imshow(av_train.reshape(28, 28), cmap=plt.cm.RdBu)\n","fig.savefig(\"SVM_3_mean_negative_support_%s_%d.png\" % (type_svm,class_i))\n"," \n","ind = 0\n","fig = plt.figure(figsize=(24,50))\n","for i, sample_id in enumerate(chosen_sample):\n","  l1 = plt.subplot(int(len(chosen_sample)/5), 5, i + 1)  \n","  sv_image = neg_supports[sample_id,:]\n","  sv_label = label_neg[sample_id]\n","  l1.imshow(sv_image.reshape(28, 28), cmap=plt.cm.RdBu)\n","  l1.set_xticks(())\n","  l1.set_yticks(())\n","  l1.set_xlabel('Sep/Class %d : Sample %d, label %s' % (class_i, i, str(sv_label))) \n","\n","fig.savefig(\"SVM_3_negative_support_%s_%d.png\" % (type_svm,class_i))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yrny65oTySut"},"outputs":[],"source":["# <<<<<<<<<<<<<  Q3.C  Calculate the histogram of the labels associated with the positive and negative supports \n","pos_hist, pos_bins =   #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","neg_hist, neg_bins =   #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","\n","width = 0.8\n","ticks = np.arange(10)\n","\n","fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4)) \n","ax = axes[0]\n","ax.bar(range(10), pos_hist, width )\n","ax.set_xticks(ticks)\n","ax.set_ylabel('Frequency')\n","ax.set_xlabel('Label')\n","ax.set_title(\"Positive Supports' Labels\")\n"," \n","ax = axes[1] \n","ax.bar(range(10), neg_hist, width )\n","ax.set_xticks(ticks)\n","ax.set_ylabel('Frequency')\n","ax.set_xlabel('Label')\n","ax.set_title(\"Negative Supports' Labels\")\n","\n","plt.show()\n","fig.savefig(\"SVM_3_MNIST_neg_pos_distribution_%s_%d.png\" % (type_svm,class_i))"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"mysvm","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}